{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vsvishnu13/cv25/blob/main/CV_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmWDjtSBVZL3"
      },
      "source": [
        "# **Assignment - 3**\n",
        "## **Fine-tuning a Pretrained ResNet Model for Image Classification**\n",
        "\n",
        "In this assignment, we will explore the concept of transfer learning by fine-tuning a pretrained ResNet model for the task of image classification. ResNet, or Residual Network, is a deep convolutional neural network architecture that has achieved state-of-the-art performance on a variety of visual recognition tasks. Pretrained models are models that have been previously trained on large datasets, such as ImageNet, and can serve as a starting point for new tasks by leveraging learned features. By fine-tuning the pretrained ResNet, we aim to adapt its feature representations to a new dataset, improving both training efficiency and model performance. This process involves adjusting the modelâ€™s final layers to suit the specific classification problem, while retaining the learned features from earlier layers. At the end we compare the prformance of the pre-trained model with that of the fine-tuned model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMEcufeVZL5"
      },
      "source": [
        "===================================================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWWAuy66VZL6"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 1**\n",
        "1) <blue>**torch**</blue>: This imports the <green>**PyTorch library**</green>, which is essential for building and training neural networks.\n",
        "2) <blue>**torch.nn**</blue>: Imports the <green>**neural network module**</green> of PyTorch, which contains various layers and loss functions used to build models.\n",
        "3) <blue>**torch.optim**</blue>: This imports the <green>**optimization algorithms**</green> module, used for updating the model's weights based on the computed gradients during training.\n",
        "4) <blue>**torchvision -> datasets, models, transforms**</blue>: Imports functions from <green>**Torchvision**</green> for accessing popular datasets, pre-trained models, and transformations for image preprocessing.\n",
        "5) <blue>**torchvision.utils -> make_grid**</blue>: This function is used to <green>**visualize multiple images**</green> in a grid format, useful for displaying image batches.\n",
        "6) <blue>**torch.utils.data -> DataLoader**</blue>: Imports the <green>**DataLoader**</green> class, which allows for efficient loading and batching of datasets during training and testing.\n",
        "7) <blue>**matplotlib.pyplot**</blue>: This imports <green>**Matplotlib**</green>, which is used for plotting and visualizing images or training metrics such as loss and accuracy.\n",
        "8) <blue>**numpy**</blue>: Imports <green>**NumPy**</green> for handling arrays and performing numerical operations, which are often useful in preprocessing and analyzing data.\n",
        "9) <blue>**time**</blue>: This allows tracking <green>**elapsed time**</green> during the training process, often used to measure performance.\n",
        "10) <blue>**os**</blue>: Imports <green>**OS module**</green> to handle tasks like creating directories, managing file paths, or interacting with the file system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE4RL4c3VZL7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj_Z5dUEVZL8"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 2**\n",
        "1) <blue>**device**</blue>: This variable checks if a <green>**CUDA-capable GPU**</green> is available; if so, it sets the device to GPU (cuda), otherwise, it falls back to the <green>**CPU**</green>.\n",
        "2) <blue>**transforms.Compose**</blue>: This creates a series of <green>**transformations for training images**</green>, applied in the specified order to augment the data and prepare it for the model.\n",
        "3) <blue>**transforms.Grayscale(num_output_channels=3)**</blue>: This transformation converts input images from grayscale to <green>**RGB**</green> by creating 3 output channels, which is necessary for models expecting RGB inputs.\n",
        "4) <blue>**transforms.RandomResizedCrop(224)**</blue>: This randomly crops the input image to a size of <green>**224x224 pixels**</green>, introducing variation in the training data to improve model generalization.\n",
        "5) <blue>**transforms.RandomHorizontalFlip()**</blue>: This randomly flips the image horizontally with a 50% chance, providing additional <green>**data augmentation**</green> for the training dataset.\n",
        "6) <blue>**transforms.ToTensor()**</blue>: Converts the PIL image or NumPy array into a <green>**PyTorch tensor**</green>, which is the required input format for models.\n",
        "7) <blue>**transforms.Normalize()**</blue>: This normalizes the tensor with mean and standard deviation values specified for each channel, helping the model to <green>**converge faster**</green> and perform better by ensuring the input data has a consistent scale.\n",
        "8) <blue>**transforms.Resize(256)**</blue>: This resizes the input image to a height and width of <green>**256 pixels**</green>, ensuring consistency in input size before cropping.\n",
        "9) <blue>**transforms.CenterCrop(224)**</blue>: This crops the center of the image to <green>**224x224 pixels**</green>, focusing on the most important part of the image for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD6Zm6E-VZL9"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transformations for training and testing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels (RGB)\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels (RGB)\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGmeb3-nVZL9"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 3**\n",
        "1) This code outlines loading the <blue>**Caltech-256**</blue> dataset for <green>**image classification**</green>.\n",
        "2) The <blue>**train_loader**</blue> and <blue>**test_loader**</blue> are used to load the datasets in batches of 32, with shuffling applied only to the training set.\n",
        "\n",
        "Note: The placeholders **\"None\"** need to be replaced with the correct dataset-loading code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfWY0ES2VZL-"
      },
      "outputs": [],
      "source": [
        "# TODO: Use Caltech-256 dataset, you can download and load it through torchvision.datasets.ImageFolder\n",
        "train_dataset = None  # Replace None with code to load the dataset with training transformations defined above\n",
        "test_dataset = None  # Replace None with code to load the dataset with test transformations defined above\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weYn3b6eVZL-"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 4**\n",
        "1) <blue>**model = None**</blue>: This line will load a <green>**pre-trained ResNet-18 model**</green> from the Torchvision library, which has been trained on the ImageNet dataset, allowing for transfer learning, when None is replaced with proper code.\n",
        "2) <blue>**for param in model.parameters()**:</blue>: This loop iterates over all the parameters of the model, enabling modifications to their properties.\n",
        "3) <blue>**param.requires_grad = None**</blue>: Inside the loop, this line will be used to <green>**freeze the model weights**</green> for all layers except the final fully connected layer. By setting requires_grad to False, gradients will not be calculated for these parameters during backpropagation, preventing them from being updated during training, when None is replace by proper code\n",
        "4) <blue>**num_ftrs = model.fc.in_features**</blue>: This line retrieves the number of input features from the final fully connected layer of the model, which is necessary for modifying that layer.\n",
        "5) <blue>**model.fc = nn.Linear(num_ftrs, 257)**</blue>: This modifies the final fully connected layer to output <green>**257 classes**</green>, corresponding to the number of classes in the Caltech-256 dataset, replacing the original output layer.\n",
        "\n",
        "Note: The placeholders **\"None\"** need to be replaced with the correct code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT-9z-btVZL-"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace None with correct code for loading a pretrained Resnet-18 model\n",
        "model = None\n",
        "\n",
        "# Freeze the model weights except for the final layer\n",
        "for param in model.parameters():\n",
        "    # TODO: Freeze the model weights\n",
        "    param.requires_grad = None\n",
        "\n",
        "# Modify the final fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 257)  # 257 classes in Caltech-256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vMQCftrVZL-"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 5**\n",
        "1) <blue>**model = None**</blue>: This line sends the <green>**model to the appropriate device**</green>, whether it's a GPU (if available) or a CPU, ensuring that the computations happen on the selected hardware, when None is replace with correct code.\n",
        "2) <blue>**criterion = None**</blue>: Defines the <green>**loss function**</green> as cross-entropy loss, which is commonly used for multi-class classification problems, when None is replaced with the correct code.\n",
        "3) <blue>**optimizer = None**</blue>: Initialize the <green>**Adam optimizer**</green> to update only the parameters of the modified final layer (model.fc). Set the learning rate to <green>**0.001**</green> to control the step size for each update during training.\n",
        "\n",
        "Note: The placeholders **\"None\"** need to be replaced with the correct code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6UlAZOcVZL-"
      },
      "outputs": [],
      "source": [
        "# TODO: Replace None with correct code to send model to device\n",
        "model = None\n",
        "\n",
        "# Define loss function and optimizer\n",
        "# TODO: Use a suitable loss criterion and optimizer with learning rate 0.001\n",
        "criterion = None\n",
        "optimizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGVoeKEoVZL_"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 6**\n",
        "1) <blue>**train_model**</blue>: This function trains and evaluates the model for a specified number of <green>**epochs**</green>.\n",
        "2) It <green>**alternates**</green> between training and testing phases, setting the model to <blue>**train() or eval()**</blue> mode accordingly.\n",
        "3) <blue>**images = None, labels = None**</blue>: Images and labels are moved to the <green>**device (CPU or GPU)**</green>.\n",
        "4) <blue>**Backpropagation and Optimization**</blue>: Gradients are calculated using backpropagation, and the optimizer <green>**updates the model parameters**</green> based on these gradients.\n",
        "\n",
        "Note: The placeholders **\"None\"** need to be replaced with the correct code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jATsRx37VZL_"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the model\n",
        "def train_model(model, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and testing phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluation mode\n",
        "                dataloader = test_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloader:\n",
        "                # TODO: Move images and labels to the device\n",
        "                images = None  # Replace None with the correct code\n",
        "                labels = None\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # TODO: Forward Pass\n",
        "                    outputs = None\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = None # Replace None with the correct code to find error between labels and outputs\n",
        "\n",
        "                    # Backward + optimize only in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tuned_model = train_model(model, criterion, optimizer, num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi6mRos_VZL_"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 7**\n",
        "The provided code consists of three functions designed for model evaluation and visualization:\n",
        "1) <blue>**visualize_predictions**</blue>: This function takes a <green>**trained model**</green> and a <green>**dataloader**</green>, then displays a set number of images with their predicted labels. It sets the model to <blue>**evaluation mode**</blue> and processes the input data without <green>**calculating gradients**</green> (using <blue>**torch.no_grad()**</blue>), which speeds up the evaluation. After moving the inputs to the correct device, it computes predictions using a forward pass through the model and visualizes the results in a subplot.\n",
        "2) <blue>**denormalize**</blue>: This utility function converts a <blue>**normalized image tensor**</blue> back to its <blue>**original form**</blue> by <green>**reversing the normalization process**</green>. It applies the <green>**mean**</green> and <green>**standard deviation**</green> used during image preprocessing, ensuring the values are scaled back into the range suitable for visualization (<blue>**clipped between 0 and 1**</blue>).\n",
        "3) <blue>**make_map_classes**</blue>: This function generates a <blue>**dictionary mapping class indices**</blue> to their corresponding class names. It reads the folder names (assuming they follow a specific naming convention), extracts the index and name of each class, and sorts them in the correct order for use in classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgGEJDi4VZL_"
      },
      "outputs": [],
      "source": [
        "# Evaluate the performance of the model before and after fine-tuning\n",
        "# Function to visualize images and their predictions\n",
        "def visualize_predictions(model, dataloader, num_images=2):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            # TODO: Move images to the device\n",
        "            inputs = None\n",
        "\n",
        "            # TODO: Forward Pass = Get Predictions\n",
        "            outputs = None\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'Predicted: {preds[i].item()}')\n",
        "                img = inputs.cpu().data[i]\n",
        "                img = img.permute(1, 2, 0).numpy()  # Convert from Tensor format\n",
        "                img = np.clip(img, 0, 1)  # Clip the values for display\n",
        "                ax.imshow(img)\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    return\n",
        "\n",
        "# Function to denormalize the image for visualization\n",
        "def denormalize(image_tensor):\n",
        "    # Means and stds used for normalization\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert tensor to HWC\n",
        "    image = std * image + mean  # Denormalize\n",
        "    image = np.clip(image, 0, 1)  # Clip values to be in [0, 1] range\n",
        "    return image\n",
        "\n",
        "\n",
        "def make_map_classes(path):\n",
        "    classes_dir = os.listdir(path)\n",
        "    classes_dir.sort()\n",
        "    classes_dict = {}\n",
        "    for c in classes_dir:\n",
        "        index = int(c[:3])\n",
        "        name = c[4:]\n",
        "        classes_dict[index] = name\n",
        "\n",
        "    return classes_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okomD9r_VZL_"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 8**\n",
        "1) This function, <blue>**compare_predictions**</blue>, is designed to visually compare the predictions made by a <green>**pre-trained model**</green> and a <green>**fine-tuned version**</green> of that model, helping to assess the impact of fine-tuning. It takes in both models, a dataloader, the directory path containing class labels, and the number of images to display.\n",
        "2) <blue>**Model Evaluation Mode**</blue>: Both the <green>**pre-trained**</green> and <green>**fine-tuned models**</green> are set to <green>**evaluation mode**</green> using <blue>**.eval()**</blue>. This ensures that no gradients are calculated during the forward pass, making inference faster and memory-efficient.\n",
        "3)  The TODO lines highlight the need to move the <blue>**input images (inputs)**</blue> to the appropriate device (<green>**CPU**</green> or <green>**GPU**</green>), though this is left as an implementation detail.\n",
        "4) The first subplot shows the prediction from the <blue>**pre-trained model**</blue> along with its <green>**corresponding class name (retrieved from**</green> the <blue>**classes_dict dictionary**</blue>). The second subplot shows the prediction from the <blue>**fine-tuned model**</blue> for comparison. This allows easy visual inspection of how fine-tuning has affected the model's ability to make accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCBxKVubVZL_"
      },
      "outputs": [],
      "source": [
        "# Function to compare predictions of non-finetuned and finetuned models\n",
        "def compare_predictions(pretrained_model, finetuned_model, dataloader, path_dir, num_images=5):\n",
        "    pretrained_model.eval()\n",
        "    finetuned_model.eval()\n",
        "\n",
        "    images_shown = 0\n",
        "    classes_dict = make_map_classes(path_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            # TODO: Move inputs to device\n",
        "            inputs = None\n",
        "\n",
        "            # Predictions from pre-trained model (without fine-tuning)\n",
        "            # TODO: Calculate outputs using forward pass of pretrained model\n",
        "            pre_outputs = None\n",
        "            _, pre_preds = torch.max(pre_outputs, 1)\n",
        "\n",
        "            # Predictions from fine-tuned model\n",
        "            # TODO: Calculate the outputs using the gforward pass of the finetuned model\n",
        "            fin_outputs = None\n",
        "            _, fin_preds = torch.max(fin_outputs, 1)\n",
        "\n",
        "            for i in range(inputs.size()[0]):\n",
        "                if images_shown == num_images:\n",
        "                    return  # Stop after showing num_images\n",
        "                images_shown += 1\n",
        "\n",
        "                img = denormalize(inputs.cpu().data[i])\n",
        "\n",
        "                plt.figure(figsize=(10, 4))\n",
        "\n",
        "                # Show pre-trained model's prediction\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.imshow(img)\n",
        "                plt.title(f'Pre-trained Prediction: {pre_preds[i].item()} : {classes_dict[pre_preds[i].item()]}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Show fine-tuned model's prediction\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.imshow(img)\n",
        "                plt.title(f'Fine-tuned Prediction: {fin_preds[i].item()} : {classes_dict[fin_preds[i].item()+1]}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EANOfKzVZL_"
      },
      "source": [
        "<style>\n",
        "blue {\n",
        "  color: skyblue;\n",
        "}\n",
        "\n",
        "red {\n",
        "  color: red;\n",
        "}\n",
        "\n",
        "green {\n",
        "  color: lightgreen;\n",
        "}\n",
        "</style>\n",
        "\n",
        "### **Step - 9**\n",
        "This code snippet is used to <green>**evaluate**</green> and visualize the predictions of both a <blue>**pre-trained**</blue> and a <blue>**fine-tuned model**</blue> on a set of example images. It compares the predictions side by side to assess the <green>**performance improvements**</green> brought by fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBbfEZG0VZL_"
      },
      "outputs": [],
      "source": [
        "# Evaluate and visualize predictions\n",
        "print(\"Comparing predictions of pre-trained and fine-tuned models on example images...\")\n",
        "\n",
        "# Ensure both models have the same structure (so, freeze layers for pretrained_model)\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 257)\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# Compare predictions using a couple of test images\n",
        "# TODO: Fill in the path of the 256_ObjectCategories directory in the path variable below.\n",
        "path_to256_ObjectCategories_dir = None\n",
        "compare_predictions(pretrained_model, fine_tuned_model, test_loader, path_to256_ObjectCategories_dir ,num_images=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For students attempting Advanced Level (optional for basic)"
      ],
      "metadata": {
        "id": "fGdCvacoZKZr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a935c4a"
      },
      "source": [
        "### **Step - 10**\n",
        "\n",
        "#### **Exploring Learning Rate Scheduling and Optimization Strategies**\n",
        "\n",
        "This step delves deeper into fine-tuning by exploring advanced techniques for optimizing the training process. Instead of using a fixed learning rate, you can implement learning rate scheduling. This involves dynamically adjusting the learning rate during training, which can help the model converge more effectively. Common strategies include step decay, exponential decay, or using reduce-on-LRPatience.\n",
        "\n",
        "Furthermore, you can experiment with different optimization algorithms beyond Adam, such as SGD with momentum, RMSprop, or even more advanced adaptive optimizers like RAdam or Lookahead. Each optimizer has its own characteristics and can perform differently depending on the dataset and model architecture.\n",
        "\n",
        "By implementing and comparing various learning rate schedules and optimization strategies, you can gain a deeper understanding of how these factors impact the fine-tuning process and model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c128ad7f"
      },
      "source": [
        "# Define a new optimizer (e.g., SGD)\n",
        "from torch.optim import SGD\n",
        "new_optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Define a learning rate scheduler (e.g., StepLR)\n",
        "from torch.optim import lr_scheduler\n",
        "new_scheduler = lr_scheduler.StepLR(new_optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Modify the train_model function to include the scheduler\n",
        "def train_model_with_scheduler(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and testing phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluation mode\n",
        "                dataloader = test_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloader:\n",
        "                # Move images and labels to the device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # Step the scheduler after each epoch\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "# Train the model with the new optimizer and scheduler\n",
        "fine_tuned_model_with_scheduler = train_model_with_scheduler(model, criterion, new_optimizer, new_scheduler, num_epochs=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e150150"
      },
      "source": [
        "### **Step - 11**\n",
        "\n",
        "#### **Investigating Different Freezing Strategies and Layer Unfreezing**\n",
        "\n",
        "In the initial fine-tuning steps, we froze all layers except the final fully connected layer. However, depending on the dataset and the similarity between the pre-training task and the fine-tuning task, you might benefit from unfreezing more layers or using different freezing strategies.\n",
        "\n",
        "This step encourages you to experiment with unfreezing earlier layers of the pre-trained model. While unfreezing all layers might lead to overfitting on smaller datasets, selectively unfreezing blocks of layers or even individual layers can allow the model to learn more task-specific features from the earlier convolutional layers.\n",
        "\n",
        "You can also explore gradual unfreezing, where you start by unfreezing only the last few layers and then progressively unfreeze earlier layers as training progresses. This can help stabilize training and prevent catastrophic forgetting of the pre-trained knowledge. By comparing the performance of models fine-tuned with different freezing strategies, you can determine the optimal approach for your specific task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc489614"
      },
      "source": [
        "# Unfreeze the last few layers (e.g., layer4 and the fully connected layer)\n",
        "def unfreeze_last_layers(model, num_layers_to_unfreeze=2):\n",
        "    # Unfreeze the fully connected layer (already unfrozen in Step 4, but good to be explicit)\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Unfreeze the last 'num_layers_to_unfreeze' convolutional layers\n",
        "    # ResNet-18 has layers: conv1, layer1, layer2, layer3, layer4, fc\n",
        "    # We want to unfreeze layer4 and potentially layer3\n",
        "    unfrozen_layers = [model.layer4, model.layer3] # Add more layers to this list as needed\n",
        "\n",
        "    for i in range(min(num_layers_to_unfreeze, len(unfrozen_layers))):\n",
        "        for param in unfrozen_layers[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# Unfreeze all layers (use with caution, especially on small datasets)\n",
        "def unfreeze_all_layers(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'model' is the pretrained ResNet-18 loaded in Step 4\n",
        "# First, freeze all layers as in Step 4\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Then, choose a strategy:\n",
        "# unfreeze_last_layers(model, num_layers_to_unfreeze=2) # Unfreeze layer4 and layer3\n",
        "# OR\n",
        "# unfreeze_all_layers(model) # Unfreeze all layers\n",
        "\n",
        "# After unfreezing, you would re-define your optimizer to include the newly unfrozen layers\n",
        "# new_optimizer_for_unfrozen = optim.Adam(model.parameters(), lr=0.0001) # Lower learning rate is often recommended\n",
        "# Then train the model again with the new optimizer\n",
        "# fine_tuned_model_unfrozen = train_model(model, criterion, new_optimizer_for_unfrozen, num_epochs=5)\n",
        "\n",
        "# You would then evaluate and compare this model's performance as done in Step 9.\n",
        "# Train and Evaluate the new model on the previously given dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ea86a0"
      },
      "source": [
        "### **Evaluatory Task (For Advanced Level)**\n",
        "\n",
        "Now that you have explored different optimization strategies, learning rate scheduling, and layer unfreezing techniques, try combining these concepts to train a new model.\n",
        "\n",
        "1.  **Choose an unfreezing strategy:** Decide which layers you want to unfreeze based on your understanding from Step 11. You can use the provided `unfreeze_last_layers` or `unfreeze_all_layers` functions, or experiment with unfreezing specific layers.\n",
        "2.  **Select an optimizer and learning rate scheduler:** Choose an optimizer (e.g., SGD, Adam with a different learning rate) and a learning rate scheduler (e.g., StepLR, ReduceLROnPlateau) from Step 10.\n",
        "3.  **Combine and train:** Integrate your chosen unfreezing strategy, optimizer, and scheduler into a training loop. You can adapt the `train_model_with_scheduler` function provided in Step 10.\n",
        "4.  **Evaluate and compare:** After training, evaluate the performance of your new model using the `visualize_predictions` and `compare_predictions` functions from Step 7 and 8. Compare its performance to the models trained in the previous steps to see the impact of your chosen techniques.\n",
        "\n",
        "Experiment with different combinations of unfreezing strategies, optimizers, and schedulers to find the best approach for fine-tuning the ResNet model on the Caltech-256 dataset.\n",
        "\n",
        "You have to write code in a new code cell with the given modifications and evaluate the model again. You can use the previous format of code written. Also include a short written discussion summarizing your findings."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDV-WhiahZSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.undefined"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}